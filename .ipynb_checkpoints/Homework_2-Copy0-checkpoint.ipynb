{
 "metadata": {
  "name": "",
  "signature": "sha256:74947bc410a41c858cf6fd1d5e402c2494705d3f764c00d91111d79a50680ad9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Senalka McDonald\n",
      "Homework #2 KNN\n",
      "COPY\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "from sklearn import datasets\n",
      "from __future__ import division\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from seaborn import plt\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.datasets import load_iris\n",
      "from sklearn import neighbors, datasets\n",
      "\n",
      "from matplotlib.colors import ListedColormap\n",
      "from sklearn import neighbors, datasets, feature_selection\n",
      "from sklearn.cross_validation import train_test_split, cross_val_score\n",
      "\n",
      "#This is what #1 is asking you to do\n",
      "from sklearn.neighbors import KNeighborsClassifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1. Implement KNN classification, using the sklearn package. We learned how to do this in class.\n",
      "See also: http://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "iris = load_iris()\n",
      "iris_keys = pd.DataFrame(iris.data, columns = iris.feature_names,)\n",
      "iris_keys['Target'] = iris.target\n",
      "iris_keys[\"target_names\"] = iris_keys.Target.map({0: 'setosa', 1: 'versicolor', 3: 'virginica'})\n",
      "#iris_keys['species'] = [iris.target_names[x] for x in iris.target]\n",
      "iris_keys \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>sepal length (cm)</th>\n",
        "      <th>sepal width (cm)</th>\n",
        "      <th>petal length (cm)</th>\n",
        "      <th>petal width (cm)</th>\n",
        "      <th>Target</th>\n",
        "      <th>target_names</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0  </th>\n",
        "      <td> 5.1</td>\n",
        "      <td> 3.5</td>\n",
        "      <td> 1.4</td>\n",
        "      <td> 0.2</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1  </th>\n",
        "      <td> 4.9</td>\n",
        "      <td> 3.0</td>\n",
        "      <td> 1.4</td>\n",
        "      <td> 0.2</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2  </th>\n",
        "      <td> 4.7</td>\n",
        "      <td> 3.2</td>\n",
        "      <td> 1.3</td>\n",
        "      <td> 0.2</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3  </th>\n",
        "      <td> 4.6</td>\n",
        "      <td> 3.1</td>\n",
        "      <td> 1.5</td>\n",
        "      <td> 0.2</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4  </th>\n",
        "      <td> 5.0</td>\n",
        "      <td> 3.6</td>\n",
        "      <td> 1.4</td>\n",
        "      <td> 0.2</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5  </th>\n",
        "      <td> 5.4</td>\n",
        "      <td> 3.9</td>\n",
        "      <td> 1.7</td>\n",
        "      <td> 0.4</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6  </th>\n",
        "      <td> 4.6</td>\n",
        "      <td> 3.4</td>\n",
        "      <td> 1.4</td>\n",
        "      <td> 0.3</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7  </th>\n",
        "      <td> 5.0</td>\n",
        "      <td> 3.4</td>\n",
        "      <td> 1.5</td>\n",
        "      <td> 0.2</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8  </th>\n",
        "      <td> 4.4</td>\n",
        "      <td> 2.9</td>\n",
        "      <td> 1.4</td>\n",
        "      <td> 0.2</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9  </th>\n",
        "      <td> 4.9</td>\n",
        "      <td> 3.1</td>\n",
        "      <td> 1.5</td>\n",
        "      <td> 0.1</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10 </th>\n",
        "      <td> 5.4</td>\n",
        "      <td> 3.7</td>\n",
        "      <td> 1.5</td>\n",
        "      <td> 0.2</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11 </th>\n",
        "      <td> 4.8</td>\n",
        "      <td> 3.4</td>\n",
        "      <td> 1.6</td>\n",
        "      <td> 0.2</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12 </th>\n",
        "      <td> 4.8</td>\n",
        "      <td> 3.0</td>\n",
        "      <td> 1.4</td>\n",
        "      <td> 0.1</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13 </th>\n",
        "      <td> 4.3</td>\n",
        "      <td> 3.0</td>\n",
        "      <td> 1.1</td>\n",
        "      <td> 0.1</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14 </th>\n",
        "      <td> 5.8</td>\n",
        "      <td> 4.0</td>\n",
        "      <td> 1.2</td>\n",
        "      <td> 0.2</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>15 </th>\n",
        "      <td> 5.7</td>\n",
        "      <td> 4.4</td>\n",
        "      <td> 1.5</td>\n",
        "      <td> 0.4</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>16 </th>\n",
        "      <td> 5.4</td>\n",
        "      <td> 3.9</td>\n",
        "      <td> 1.3</td>\n",
        "      <td> 0.4</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>17 </th>\n",
        "      <td> 5.1</td>\n",
        "      <td> 3.5</td>\n",
        "      <td> 1.4</td>\n",
        "      <td> 0.3</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>18 </th>\n",
        "      <td> 5.7</td>\n",
        "      <td> 3.8</td>\n",
        "      <td> 1.7</td>\n",
        "      <td> 0.3</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>19 </th>\n",
        "      <td> 5.1</td>\n",
        "      <td> 3.8</td>\n",
        "      <td> 1.5</td>\n",
        "      <td> 0.3</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>20 </th>\n",
        "      <td> 5.4</td>\n",
        "      <td> 3.4</td>\n",
        "      <td> 1.7</td>\n",
        "      <td> 0.2</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>21 </th>\n",
        "      <td> 5.1</td>\n",
        "      <td> 3.7</td>\n",
        "      <td> 1.5</td>\n",
        "      <td> 0.4</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>22 </th>\n",
        "      <td> 4.6</td>\n",
        "      <td> 3.6</td>\n",
        "      <td> 1.0</td>\n",
        "      <td> 0.2</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>23 </th>\n",
        "      <td> 5.1</td>\n",
        "      <td> 3.3</td>\n",
        "      <td> 1.7</td>\n",
        "      <td> 0.5</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>24 </th>\n",
        "      <td> 4.8</td>\n",
        "      <td> 3.4</td>\n",
        "      <td> 1.9</td>\n",
        "      <td> 0.2</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25 </th>\n",
        "      <td> 5.0</td>\n",
        "      <td> 3.0</td>\n",
        "      <td> 1.6</td>\n",
        "      <td> 0.2</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>26 </th>\n",
        "      <td> 5.0</td>\n",
        "      <td> 3.4</td>\n",
        "      <td> 1.6</td>\n",
        "      <td> 0.4</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>27 </th>\n",
        "      <td> 5.2</td>\n",
        "      <td> 3.5</td>\n",
        "      <td> 1.5</td>\n",
        "      <td> 0.2</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>28 </th>\n",
        "      <td> 5.2</td>\n",
        "      <td> 3.4</td>\n",
        "      <td> 1.4</td>\n",
        "      <td> 0.2</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>29 </th>\n",
        "      <td> 4.7</td>\n",
        "      <td> 3.2</td>\n",
        "      <td> 1.6</td>\n",
        "      <td> 0.2</td>\n",
        "      <td> 0</td>\n",
        "      <td> setosa</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>...</th>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>120</th>\n",
        "      <td> 6.9</td>\n",
        "      <td> 3.2</td>\n",
        "      <td> 5.7</td>\n",
        "      <td> 2.3</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>121</th>\n",
        "      <td> 5.6</td>\n",
        "      <td> 2.8</td>\n",
        "      <td> 4.9</td>\n",
        "      <td> 2.0</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>122</th>\n",
        "      <td> 7.7</td>\n",
        "      <td> 2.8</td>\n",
        "      <td> 6.7</td>\n",
        "      <td> 2.0</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>123</th>\n",
        "      <td> 6.3</td>\n",
        "      <td> 2.7</td>\n",
        "      <td> 4.9</td>\n",
        "      <td> 1.8</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>124</th>\n",
        "      <td> 6.7</td>\n",
        "      <td> 3.3</td>\n",
        "      <td> 5.7</td>\n",
        "      <td> 2.1</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>125</th>\n",
        "      <td> 7.2</td>\n",
        "      <td> 3.2</td>\n",
        "      <td> 6.0</td>\n",
        "      <td> 1.8</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>126</th>\n",
        "      <td> 6.2</td>\n",
        "      <td> 2.8</td>\n",
        "      <td> 4.8</td>\n",
        "      <td> 1.8</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>127</th>\n",
        "      <td> 6.1</td>\n",
        "      <td> 3.0</td>\n",
        "      <td> 4.9</td>\n",
        "      <td> 1.8</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>128</th>\n",
        "      <td> 6.4</td>\n",
        "      <td> 2.8</td>\n",
        "      <td> 5.6</td>\n",
        "      <td> 2.1</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>129</th>\n",
        "      <td> 7.2</td>\n",
        "      <td> 3.0</td>\n",
        "      <td> 5.8</td>\n",
        "      <td> 1.6</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>130</th>\n",
        "      <td> 7.4</td>\n",
        "      <td> 2.8</td>\n",
        "      <td> 6.1</td>\n",
        "      <td> 1.9</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>131</th>\n",
        "      <td> 7.9</td>\n",
        "      <td> 3.8</td>\n",
        "      <td> 6.4</td>\n",
        "      <td> 2.0</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>132</th>\n",
        "      <td> 6.4</td>\n",
        "      <td> 2.8</td>\n",
        "      <td> 5.6</td>\n",
        "      <td> 2.2</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>133</th>\n",
        "      <td> 6.3</td>\n",
        "      <td> 2.8</td>\n",
        "      <td> 5.1</td>\n",
        "      <td> 1.5</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>134</th>\n",
        "      <td> 6.1</td>\n",
        "      <td> 2.6</td>\n",
        "      <td> 5.6</td>\n",
        "      <td> 1.4</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>135</th>\n",
        "      <td> 7.7</td>\n",
        "      <td> 3.0</td>\n",
        "      <td> 6.1</td>\n",
        "      <td> 2.3</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>136</th>\n",
        "      <td> 6.3</td>\n",
        "      <td> 3.4</td>\n",
        "      <td> 5.6</td>\n",
        "      <td> 2.4</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>137</th>\n",
        "      <td> 6.4</td>\n",
        "      <td> 3.1</td>\n",
        "      <td> 5.5</td>\n",
        "      <td> 1.8</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>138</th>\n",
        "      <td> 6.0</td>\n",
        "      <td> 3.0</td>\n",
        "      <td> 4.8</td>\n",
        "      <td> 1.8</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>139</th>\n",
        "      <td> 6.9</td>\n",
        "      <td> 3.1</td>\n",
        "      <td> 5.4</td>\n",
        "      <td> 2.1</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>140</th>\n",
        "      <td> 6.7</td>\n",
        "      <td> 3.1</td>\n",
        "      <td> 5.6</td>\n",
        "      <td> 2.4</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>141</th>\n",
        "      <td> 6.9</td>\n",
        "      <td> 3.1</td>\n",
        "      <td> 5.1</td>\n",
        "      <td> 2.3</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>142</th>\n",
        "      <td> 5.8</td>\n",
        "      <td> 2.7</td>\n",
        "      <td> 5.1</td>\n",
        "      <td> 1.9</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>143</th>\n",
        "      <td> 6.8</td>\n",
        "      <td> 3.2</td>\n",
        "      <td> 5.9</td>\n",
        "      <td> 2.3</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>144</th>\n",
        "      <td> 6.7</td>\n",
        "      <td> 3.3</td>\n",
        "      <td> 5.7</td>\n",
        "      <td> 2.5</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>145</th>\n",
        "      <td> 6.7</td>\n",
        "      <td> 3.0</td>\n",
        "      <td> 5.2</td>\n",
        "      <td> 2.3</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>146</th>\n",
        "      <td> 6.3</td>\n",
        "      <td> 2.5</td>\n",
        "      <td> 5.0</td>\n",
        "      <td> 1.9</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>147</th>\n",
        "      <td> 6.5</td>\n",
        "      <td> 3.0</td>\n",
        "      <td> 5.2</td>\n",
        "      <td> 2.0</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>148</th>\n",
        "      <td> 6.2</td>\n",
        "      <td> 3.4</td>\n",
        "      <td> 5.4</td>\n",
        "      <td> 2.3</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>149</th>\n",
        "      <td> 5.9</td>\n",
        "      <td> 3.0</td>\n",
        "      <td> 5.1</td>\n",
        "      <td> 1.8</td>\n",
        "      <td> 2</td>\n",
        "      <td>    NaN</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>150 rows \u00d7 6 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
        "0                  5.1               3.5                1.4               0.2   \n",
        "1                  4.9               3.0                1.4               0.2   \n",
        "2                  4.7               3.2                1.3               0.2   \n",
        "3                  4.6               3.1                1.5               0.2   \n",
        "4                  5.0               3.6                1.4               0.2   \n",
        "5                  5.4               3.9                1.7               0.4   \n",
        "6                  4.6               3.4                1.4               0.3   \n",
        "7                  5.0               3.4                1.5               0.2   \n",
        "8                  4.4               2.9                1.4               0.2   \n",
        "9                  4.9               3.1                1.5               0.1   \n",
        "10                 5.4               3.7                1.5               0.2   \n",
        "11                 4.8               3.4                1.6               0.2   \n",
        "12                 4.8               3.0                1.4               0.1   \n",
        "13                 4.3               3.0                1.1               0.1   \n",
        "14                 5.8               4.0                1.2               0.2   \n",
        "15                 5.7               4.4                1.5               0.4   \n",
        "16                 5.4               3.9                1.3               0.4   \n",
        "17                 5.1               3.5                1.4               0.3   \n",
        "18                 5.7               3.8                1.7               0.3   \n",
        "19                 5.1               3.8                1.5               0.3   \n",
        "20                 5.4               3.4                1.7               0.2   \n",
        "21                 5.1               3.7                1.5               0.4   \n",
        "22                 4.6               3.6                1.0               0.2   \n",
        "23                 5.1               3.3                1.7               0.5   \n",
        "24                 4.8               3.4                1.9               0.2   \n",
        "25                 5.0               3.0                1.6               0.2   \n",
        "26                 5.0               3.4                1.6               0.4   \n",
        "27                 5.2               3.5                1.5               0.2   \n",
        "28                 5.2               3.4                1.4               0.2   \n",
        "29                 4.7               3.2                1.6               0.2   \n",
        "..                 ...               ...                ...               ...   \n",
        "120                6.9               3.2                5.7               2.3   \n",
        "121                5.6               2.8                4.9               2.0   \n",
        "122                7.7               2.8                6.7               2.0   \n",
        "123                6.3               2.7                4.9               1.8   \n",
        "124                6.7               3.3                5.7               2.1   \n",
        "125                7.2               3.2                6.0               1.8   \n",
        "126                6.2               2.8                4.8               1.8   \n",
        "127                6.1               3.0                4.9               1.8   \n",
        "128                6.4               2.8                5.6               2.1   \n",
        "129                7.2               3.0                5.8               1.6   \n",
        "130                7.4               2.8                6.1               1.9   \n",
        "131                7.9               3.8                6.4               2.0   \n",
        "132                6.4               2.8                5.6               2.2   \n",
        "133                6.3               2.8                5.1               1.5   \n",
        "134                6.1               2.6                5.6               1.4   \n",
        "135                7.7               3.0                6.1               2.3   \n",
        "136                6.3               3.4                5.6               2.4   \n",
        "137                6.4               3.1                5.5               1.8   \n",
        "138                6.0               3.0                4.8               1.8   \n",
        "139                6.9               3.1                5.4               2.1   \n",
        "140                6.7               3.1                5.6               2.4   \n",
        "141                6.9               3.1                5.1               2.3   \n",
        "142                5.8               2.7                5.1               1.9   \n",
        "143                6.8               3.2                5.9               2.3   \n",
        "144                6.7               3.3                5.7               2.5   \n",
        "145                6.7               3.0                5.2               2.3   \n",
        "146                6.3               2.5                5.0               1.9   \n",
        "147                6.5               3.0                5.2               2.0   \n",
        "148                6.2               3.4                5.4               2.3   \n",
        "149                5.9               3.0                5.1               1.8   \n",
        "\n",
        "     Target target_names  \n",
        "0         0       setosa  \n",
        "1         0       setosa  \n",
        "2         0       setosa  \n",
        "3         0       setosa  \n",
        "4         0       setosa  \n",
        "5         0       setosa  \n",
        "6         0       setosa  \n",
        "7         0       setosa  \n",
        "8         0       setosa  \n",
        "9         0       setosa  \n",
        "10        0       setosa  \n",
        "11        0       setosa  \n",
        "12        0       setosa  \n",
        "13        0       setosa  \n",
        "14        0       setosa  \n",
        "15        0       setosa  \n",
        "16        0       setosa  \n",
        "17        0       setosa  \n",
        "18        0       setosa  \n",
        "19        0       setosa  \n",
        "20        0       setosa  \n",
        "21        0       setosa  \n",
        "22        0       setosa  \n",
        "23        0       setosa  \n",
        "24        0       setosa  \n",
        "25        0       setosa  \n",
        "26        0       setosa  \n",
        "27        0       setosa  \n",
        "28        0       setosa  \n",
        "29        0       setosa  \n",
        "..      ...          ...  \n",
        "120       2          NaN  \n",
        "121       2          NaN  \n",
        "122       2          NaN  \n",
        "123       2          NaN  \n",
        "124       2          NaN  \n",
        "125       2          NaN  \n",
        "126       2          NaN  \n",
        "127       2          NaN  \n",
        "128       2          NaN  \n",
        "129       2          NaN  \n",
        "130       2          NaN  \n",
        "131       2          NaN  \n",
        "132       2          NaN  \n",
        "133       2          NaN  \n",
        "134       2          NaN  \n",
        "135       2          NaN  \n",
        "136       2          NaN  \n",
        "137       2          NaN  \n",
        "138       2          NaN  \n",
        "139       2          NaN  \n",
        "140       2          NaN  \n",
        "141       2          NaN  \n",
        "142       2          NaN  \n",
        "143       2          NaN  \n",
        "144       2          NaN  \n",
        "145       2          NaN  \n",
        "146       2          NaN  \n",
        "147       2          NaN  \n",
        "148       2          NaN  \n",
        "149       2          NaN  \n",
        "\n",
        "[150 rows x 6 columns]"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Independent Variables: \\n%s' % iris.feature_names\n",
      "print '-' * 20\n",
      "print 'Class Labels: \\n%s' % iris.target_names"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Independent Variables: \n",
        "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
        "--------------------\n",
        "Class Labels: \n",
        "['setosa' 'versicolor' 'virginica']\n"
       ]
      }
     ],
     "prompt_number": 167
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_neighbors = range(1,51,2)\n",
      "np.random.seed(32)\n",
      "\n",
      "# Try this sequence again with the following random seed.\n",
      "# observe how it changes the scores of K quite dramatically\n",
      "# np.random.seed(1234)\n",
      "\n",
      "X = iris_keys.ix[:, 0:4]\n",
      "y = iris_keys.ix[:, 0:4]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "knn = KNeighborsClassifier(n_neighbors=3) #just picked a number at random. #5 is default\n",
      "knn.fit(X, y)\n",
      "\n",
      "# X = feature set (sepal length, petal....), y = classifier (target names (which are the flower names))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
        "           metric_params=None, n_neighbors=3, p=2, weights='uniform')"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create the training (and test) set using scikit-learn's train_test_split function\n",
      "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 174
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scores = []\n",
      "for n in n_neighbors:\n",
      "    clf = neighbors.KNeighborsClassifier(n)\n",
      "    clf.fit(X_train, y_train)\n",
      "    scores.append(clf.score(X_test, y_test))\n",
      "    \n",
      "plt.plot(n_neighbors, scores, linewidth=3.0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 175,
       "text": [
        "[<matplotlib.lines.Line2D at 0x1104659d0>]"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAECCAYAAAD9z2x7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8f8BtwRNWXWb0QoAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10ea75610>"
       ]
      }
     ],
     "prompt_number": 175
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"It looks like k = 11?????\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "It looks like k = 11?????\n"
       ]
      }
     ],
     "prompt_number": 177
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\ufffc\n",
      "2. Use the sklearn package to implement cross-validation for your classifier. Use 5 folds for your cross-validation.\n",
      "See also: http://scikit-learn.org/stable/modules/cross_validation.html#"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import KFold\n",
      "\n",
      "kf = KFold(5, n_folds = 5)\n",
      "for train, test in kf:\n",
      "    print(\"%s %s\" % (train, test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1 2 3 4] [0]\n",
        "[0 2 3 4] [1]\n",
        "[0 1 3 4] [2]\n",
        "[0 1 2 4] [3]\n",
        "[0 1 2 3] [4]\n"
       ]
      }
     ],
     "prompt_number": 211
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# one way to do it\n",
      "\n",
      "clf = neighbors.KNeighborsClassifier(11, weights='uniform')\n",
      "clf.fit(iris.data[:, 2:4], iris.target)\n",
      "scores = cross_val_score(clf, iris_keys.values, iris.target, cv=5)\n",
      "\n",
      "print scores\n",
      "print np.mean(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.93333333  1.          1.          0.96666667  1.        ]\n",
        "0.98\n"
       ]
      }
     ],
     "prompt_number": 218
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#another way to do it. which ones better? which ones right?\n",
      "\n",
      "scores = []\n",
      "ind = np.random.uniform(0, 1, len(X)) >= 0.3\n",
      "for n in range(5):\n",
      "    np.random.shuffle(ind)\n",
      "    X_train, X_test = X[ind], X[ind == False]\n",
      "    y_train, y_test = y[ind], y[ind == False]\n",
      "    clf = neighbors.KNeighborsClassifier(11, weights='uniform')\n",
      "    clf.fit(X_train, y_train)\n",
      "    scores.append(clf.score(X_test, y_test))\n",
      "    \n",
      "print scores\n",
      "np.mean(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.93877551020408168, 0.91836734693877553, 0.97959183673469385, 0.93877551020408168, 0.95918367346938771]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 220,
       "text": [
        "0.94693877551020411"
       ]
      }
     ],
     "prompt_number": 220
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "3. Use your KNN classifier and cross-validation code from (1) and (2) above to determine the optimal value of K (number of nearest neighbors to consult) for this Iris dataset. Hint: This hyperparameter will be a number between 1 and 150 J\uf04a."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'doesn\\'t k = 11?'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 222,
       "text": [
        "\"doesn't k = 11?\""
       ]
      }
     ],
     "prompt_number": 222
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\n",
      "4. Using matplotlib, plot classifier accuracy versus the hyperparameter K for a range of K that you consider interesting. Explain in words what you are seeing."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "5. Now, write your own implementation of cross-validation in Python without using the cross-validation methods from sklearn. Cross validation is a very important concept. Implementing it yourself in Python is the best way to learn and understand it. Compare the results of your cross-validation code with your results using the cross-validation in sklearn."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "6. EXTRA CREDIT 1: Using the value of K obtained in (3) above, vary the number of folds used for cross-validation across an interesting range, e.g. [ 2, 3, 5, 6, 10, 15]. How does classifier accuracy vary with the number of folds used? Do you think there exists an optimal number of folds to use for this particular problem? Why or why not?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "7. EXTRA CREDIT 2: Write your own implementation of KNN classification in Python, without using the methods from sklearn. Compare your results with the results you obtained using sklearn."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 0
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 0
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 0
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(__doc__)\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.colors import ListedColormap\n",
      "from sklearn import neighbors, datasets\n",
      "\n",
      "n_neighbors = 15\n",
      "\n",
      "# import some data to play with\n",
      "iris = datasets.load_iris()\n",
      "X = iris.data[:, :2]  # we only take the first two features. We could\n",
      "                      # avoid this ugly slicing by using a two-dim dataset\n",
      "y = iris.target\n",
      "\n",
      "h = .02  # step size in the mesh\n",
      "\n",
      "# Create color maps\n",
      "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
      "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
      "\n",
      "for weights in ['uniform', 'distance']:\n",
      "    # we create an instance of Neighbours Classifier and fit the data.\n",
      "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
      "    clf.fit(X, y)\n",
      "\n",
      "    # Plot the decision boundary. For that, we will assign a color to each\n",
      "    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
      "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
      "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
      "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
      "                         np.arange(y_min, y_max, h))\n",
      "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
      "\n",
      "    # Put the result into a color plot\n",
      "    Z = Z.reshape(xx.shape)\n",
      "    plt.figure()\n",
      "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
      "\n",
      "    # Plot also the training points\n",
      "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
      "    plt.xlim(xx.min(), xx.max())\n",
      "    plt.ylim(yy.min(), yy.max())\n",
      "    plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n",
      "              % (n_neighbors, weights))\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Automatically created module for IPython interactive environment\n"
       ]
      }
     ],
     "prompt_number": 2
    }
   ],
   "metadata": {}
  }
 ]
}